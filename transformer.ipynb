{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figlHFY9_YFT"
      },
      "source": [
        "# Transformer for Language Translation in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ojshAO__YFY"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "For the translation task we will use the French-English dataset provided with the zip file. Make sure you extract the zip in the proper location. If the zip file is not extracted in the proper location your code will not work. You'll need to use GPUs (at Google Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKosmNvf_YFe",
        "outputId": "77d7abad-a4cc-4f22-de8e-e5faf5d8e226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwV6I41fygmX"
      },
      "source": [
        "Make sure you sure extract the zip in a directory named Transformer. Or change the following depending your directory name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErcXw-LCCU5p"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Transformer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2TimXbMIICA",
        "outputId": "29bbeb65-2170-4543-b6d2-941fcb8cd537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ0Rso_2_YFr"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from fra_eng_dataset import FraEngDataset, fra_eng_dataset_collate\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_rmo748_YFz"
      },
      "source": [
        "## Attention Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OOJn5A2zAwC"
      },
      "source": [
        "Please read the section \"Self-Attention in Detail\" in Illustracted Transformer blog post (or see the original paper) and the complete the following section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7g3bm12_YF2"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        # K V Q are the matrices for creating the keys, values and queries vector\n",
        "        self.d_model = d_model\n",
        "        self.K = nn.Linear(d_model, d_model)\n",
        "        self.V = nn.Linear(d_model, d_model)\n",
        "        self.Q = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, padding_mask = None, subsq_mask = None):\n",
        "        # x shape: [N, SEQ, D_MODEL]\n",
        "\n",
        "        keys = self.K.forward(x)\n",
        "        values = self.V.forward(x)\n",
        "        queries = self.Q.forward(x)\n",
        "\n",
        "        sqrt_d = self.d_model ** 0.5\n",
        "\n",
        "        att = torch.matmul(queries, keys.transpose(1,2)) / sqrt_d #Attention before softmax comes here\n",
        "        # att shape: [N, SEQ, SEQ]\n",
        "        # Broadcast padding mask to word attentions so that word attention does not attend to positions outside the sentence\n",
        "        if padding_mask is not None:\n",
        "            att = att + padding_mask.transpose(1,2)\n",
        "        # Add subsequent mask so that each position can attend only itself and the previous elements\n",
        "        if subsq_mask is not None:\n",
        "            att = att + subsq_mask.unsqueeze(0)\n",
        "\n",
        "        att_softmax = torch.softmax(att, dim=2)\n",
        "        # shape: [N, SEQ, SEQ]\n",
        "        att_out = torch.matmul(att_softmax, values)\n",
        "        # shape: [N, SEQ, D_MODEL]\n",
        "\n",
        "        return att_out, keys, values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ2KRdxd_YF9"
      },
      "source": [
        "### Memory Attention\n",
        "This is the same as Self Attention only the keys and values matrices are not calculated from weights and input but are passed to the forward function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7-IVvMg_YF-"
      },
      "outputs": [],
      "source": [
        "class MemAttentionHead(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.Q = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mem_padding_mask, keys = None, values = None):\n",
        "\n",
        "        # X shape: [N, SEQ, D_MODEL]\n",
        "        queries = self.Q.forward(x)\n",
        "        sqrt_d = self.d_model ** 0.5\n",
        "\n",
        "        att = torch.matmul(queries, keys.transpose(1,2)) / sqrt_d\n",
        "        # att shape: [N, SEQ_TGT, SEQ_SRC]\n",
        "\n",
        "        # Broadcast padding mask to word attentions so that word attention does not attend to positions outside the source sentence\n",
        "        if mem_padding_mask is not None:\n",
        "            att = att + mem_padding_mask.transpose(1,2)\n",
        "\n",
        "        att_softmax = torch.softmax(att, dim=2)\n",
        "        # shape: [N, SEQ_TGT, SEQ_SRC]\n",
        "        att_out = torch.matmul(att_softmax, values)\n",
        "        # shape: [N, SEQ_TGT, D_MODEL]\n",
        "\n",
        "        return att_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54YbZMvx_YGG"
      },
      "source": [
        "### Multi-head attentions for Self Attention and Memory Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4flu62Z12lw"
      },
      "source": [
        "Please read the section \"The Beast With Many Heads\" in the illustrated Transformer article before attempting this section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PggyHjKB_YGH"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(d_model) for i in range(num_heads)]) # Use the nn.ModuleList( ) function for this\n",
        "        self.linear = nn.Linear(num_heads * d_model, d_model)\n",
        "\n",
        "    def forward(self, src, src_padding_mask, src_subsq_mask):\n",
        "\n",
        "        out_cat = None\n",
        "        keys = None\n",
        "        values = None\n",
        "\n",
        "        for i in range(self.num_heads):\n",
        "            head_outp, keys, values = self.heads[i].forward(src, src_padding_mask, src_subsq_mask)\n",
        "\n",
        "            if i == 0:\n",
        "                out_cat = head_outp\n",
        "            else:\n",
        "                out_cat = torch.cat([out_cat, head_outp], dim=2)\n",
        "\n",
        "        ret = self.linear.forward(out_cat)\n",
        "\n",
        "        return ret, keys, values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QxWaTVK2hZJ"
      },
      "source": [
        "This is same as before but for memory attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46a6IG1v_YGP"
      },
      "outputs": [],
      "source": [
        "class MultiHeadMemAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.heads = nn.ModuleList([MemAttentionHead(d_model) for i in range(num_heads)])\n",
        "        self.linear = nn.Linear(num_heads * d_model, d_model)\n",
        "\n",
        "    def forward(self, src, src_padding_mask, keys, values):\n",
        "\n",
        "        out_cat = None\n",
        "        for i in range(self.num_heads):\n",
        "            head_outp = self.heads[i].forward(src, src_padding_mask, keys = keys, values = values)\n",
        "\n",
        "            if i == 0:\n",
        "                out_cat = head_outp\n",
        "            else:\n",
        "                out_cat = torch.cat([out_cat, head_outp], dim=2)\n",
        "\n",
        "        ret = self.linear.forward(out_cat)\n",
        "\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvlgDBKY_YGU"
      },
      "source": [
        "### Encoder layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gscU-jiG2ydv"
      },
      "source": [
        "Please read the section \"Residuals\" in the Illustrated Transformer article before attempting this section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGSX2YmQ_YGV"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_att_heads, ff_dim = 2048, dropout = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multihead_attention = MultiHeadSelfAttention(d_model, num_att_heads)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.att_sublayer_norm = torch.nn.LayerNorm(d_model) # Use torch.nn.LayerNorm() here\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_lin = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
        "\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.lin_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, src, src_padding_mask, src_subsq_mask):\n",
        "\n",
        "        residual_1 = src\n",
        "        x, keys, values = self.multihead_attention.forward(src, src_padding_mask, src_subsq_mask)\n",
        "        x = self.att_sublayer_norm.forward(residual_1 + self.dropout1(x)) # call att_sublayer_norm\n",
        "\n",
        "        residual_2 = x\n",
        "        x = self.linear2(self.dropout_lin(self.relu(self.linear1.forward(x))))\n",
        "        x = self.lin_sublayer_norm(residual_2 + self.dropout2(x)) # call lin_sublayer_norm\n",
        "\n",
        "        return x, keys, values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvjkp1Bb_YGc"
      },
      "source": [
        "### Decoder layer\n",
        "\n",
        "The only difference between Encoder and Decoder layer is the additional MultiHeadMemAttention() sublayer in the decoder layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7O5jz4h_YGc"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_att_heads, ff_dim = 2048, dropout = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.multihead_self_attention = MultiHeadSelfAttention(d_model, num_att_heads)\n",
        "        self.self_att_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.multihead_mem_attention = MultiHeadMemAttention(d_model, num_att_heads)\n",
        "        self.mem_att_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
        "        self.dropout_lin = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
        "        self.lin_sublayer_norm = torch.nn.LayerNorm(d_model)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, src_padding_mask, tgt_padding_mask, tgt_subsq_mask, mem_keys, mem_values):\n",
        "\n",
        "        residual_1 = x\n",
        "        x, keys, values = self.multihead_self_attention.forward(x, tgt_padding_mask, tgt_subsq_mask)\n",
        "        x = self.self_att_sublayer_norm.forward(residual_1 + self.dropout1(x))\n",
        "\n",
        "        residual_2 = x\n",
        "        x = self.multihead_mem_attention.forward(x, src_padding_mask, keys = mem_keys, values = mem_values)\n",
        "        x = self.mem_att_sublayer_norm.forward(residual_2 + self.dropout2(x))\n",
        "\n",
        "        residual_3 = x\n",
        "        x = self.linear2(self.dropout_lin(self.relu(self.linear1.forward(x))))\n",
        "        x = self.lin_sublayer_norm(residual_3 + self.dropout3(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcc0fJvy_YGi"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysTuoG-v_YGk"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_att_heads):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_att_heads) for i in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self,src, src_padding_mask, src_subsq_mask):\n",
        "        x = src\n",
        "\n",
        "        keys = None\n",
        "        values = None\n",
        "        for layer in self.layers:\n",
        "            x, keys, values = layer.forward(x, src_padding_mask, src_subsq_mask)\n",
        "\n",
        "        x = self.norm.forward(x)\n",
        "\n",
        "        return keys, values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8yCC8NW_YGq"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nBgP5wz_YGq"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_att_heads):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_att_heads) for i in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, src_padding_mask, tgt_padding_mask, tgt_subsq_mask, mem_keys, mem_values):\n",
        "        x = tgt\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x, src_padding_mask, tgt_padding_mask, tgt_subsq_mask, mem_keys, mem_values)\n",
        "\n",
        "        x = self.norm.forward(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk8mMMIh_YGx"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "Please read the section on \"Positional Encoding\" on Illustrated Transformer blogpost before attempting this\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40qMqmNP_YGz"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.sin_args = torch.zeros(1, self.d_model).to(device)\n",
        "        self.cos_args = torch.zeros(1, self.d_model).to(device)\n",
        "        for i in range(self.d_model//2):\n",
        "            self.sin_args[0,i * 2] = 10000**(2.*i/self.d_model)\n",
        "            self.cos_args[0,i * 2 + 1] = 10000**(2.*i/self.d_model)\n",
        "\n",
        "        self.sin_args_mask = (self.sin_args > 1e-10).float()\n",
        "        self.sin_args = self.sin_args + (self.sin_args < 1e-10).float()\n",
        "\n",
        "        self.cos_args_mask = (self.cos_args > 1e-10).float()\n",
        "        self.cos_args = self.cos_args + (self.cos_args < 1e-10).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for pos in range(x.size()[-2]):\n",
        "            x[:,pos,:] = x[:,pos,:] + \\\n",
        "                         torch.sin(pos / self.sin_args) * self.sin_args_mask + \\\n",
        "                         torch.cos(pos / self.cos_args) * self.cos_args_mask\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAmY7xVV_YG5"
      },
      "source": [
        "Here is a vizualization of the positional encoding vectors. Each row is an encoding for a word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "0fskAsp9_YG6",
        "outputId": "7c567869-0312-43fe-f751-a1200a09660a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "positional_enc = PositionalEncoding(128).to(device)\n",
        "data = torch.zeros(1, 50, 128).to(device)\n",
        "data_pos_enc = positional_enc.forward(data)\n",
        "\n",
        "enc_np = data_pos_enc.squeeze(dim=0).to('cpu').numpy()\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.imshow(enc_np)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig-oDR0j_YHB"
      },
      "source": [
        "## The Transformer model\n",
        "\n",
        "Now combine all the pieces to build the Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyD8JyUm_YHD"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_att_heads, input_dict_size, output_dict_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sent_len = 50\n",
        "\n",
        "        self.input_emb = nn.Embedding(input_dict_size, d_model)\n",
        "        self.outp_emb = nn.Embedding(output_dict_size, d_model)\n",
        "\n",
        "        self.positional_encoder = PositionalEncoding(d_model)\n",
        "        self.encoder = Encoder(num_layers, d_model, num_att_heads)\n",
        "        self.decoder = Decoder(num_layers, d_model, num_att_heads)\n",
        "\n",
        "        self.outp_logits = nn.Linear(d_model, output_dict_size)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def translate(self, src, tgt_start_code, tgt_eos_code, src_padding_mask, src_subsq_mask):\n",
        "\n",
        "        # This function is already complete\n",
        "\n",
        "        enc_x = self.input_emb.forward(src.squeeze(dim=2))\n",
        "        enc_x = self.positional_encoder.forward(enc_x)\n",
        "        enc_keys, enc_values = self.encoder.forward(enc_x, src_padding_mask, src_subsq_mask)\n",
        "\n",
        "        snt = torch.ones((1,1,1)) * tgt_start_code\n",
        "        snt = snt.long()\n",
        "        snt = snt.to(device)\n",
        "\n",
        "        translation_idxes = []\n",
        "\n",
        "        for idx in range(self.max_sent_len):\n",
        "\n",
        "            dec_x = self.outp_emb.forward(snt.squeeze(dim=2))\n",
        "            dec_x = self.positional_encoder.forward(dec_x)\n",
        "            dec_x = self.decoder.forward(\n",
        "                dec_x,\n",
        "                src_padding_mask = src_padding_mask,\n",
        "                tgt_padding_mask = torch.zeros_like(snt).float().to(device),\n",
        "                tgt_subsq_mask = get_square_subsequent_mask(snt.size()[1]),\n",
        "                mem_keys = enc_keys,\n",
        "                mem_values = enc_values\n",
        "            )\n",
        "            dec_x = self.outp_logits.forward(dec_x)\n",
        "            dec_x = self.softmax(dec_x)\n",
        "            next_word_softmax = dec_x[0,idx,:].to('cpu').detach()\n",
        "            next_word_idx = torch.argmax(next_word_softmax)\n",
        "            snt = torch.cat([snt, torch.ones((1,1,1)).long().to(device) * next_word_idx], dim=1)\n",
        "\n",
        "            translation_idxes.append(next_word_idx)\n",
        "\n",
        "            if next_word_idx == tgt_eos_code:\n",
        "                break\n",
        "\n",
        "        return translation_idxes\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask, src_subsq_mask, tgt_padding_mask, tgt_subsq_mask):\n",
        "\n",
        "        enc_x = self.input_emb.forward(src.squeeze(dim=2))\n",
        "        enc_x = self.positional_encoder.forward(enc_x)\n",
        "        enc_keys, enc_values = self.encoder.forward(enc_x, src_padding_mask, src_subsq_mask)\n",
        "\n",
        "        dec_x = self.outp_emb.forward(tgt.squeeze(dim=2))\n",
        "        dec_x = self.positional_encoder.forward(dec_x)\n",
        "        dec_x = self.decoder.forward(dec_x, src_padding_mask, tgt_padding_mask, tgt_subsq_mask, enc_keys, enc_values)\n",
        "        dec_x = self.outp_logits.forward(dec_x)\n",
        "        dec_x = self.softmax(dec_x)\n",
        "\n",
        "        return dec_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG3maiHu_YHJ"
      },
      "source": [
        "### Some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2I3R_QM_YHK"
      },
      "outputs": [],
      "source": [
        "def get_square_subsequent_mask(seq_len):\n",
        "    mask = (torch.triu(torch.ones(seq_len, seq_len).to(device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def get_padding_mask(input, val1 = float('-inf'), val2 = float(0.0)):\n",
        "    mask = torch.ones(input.size()).to(device)\n",
        "    mask = mask.float().masked_fill(input == 0, val1).masked_fill(input > 0, val2)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def get_one_hot(x, out_dim, mask):\n",
        "\n",
        "    tens = x.view(-1)\n",
        "    tens_one_hot = torch.zeros(list(tens.size()) + [out_dim]).to(device)\n",
        "    for i in range(len(tens)):\n",
        "        tens_one_hot[i,tens[i]] = 1\n",
        "\n",
        "    tens_one_hot = tens_one_hot.view(list(x.size()) + [out_dim])\n",
        "    tens_one_hot = tens_one_hot * mask\n",
        "    return tens_one_hot.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM4LYVeN_YHN"
      },
      "outputs": [],
      "source": [
        "#Prepare the sentence for model translate function\n",
        "def translate_sentences(src_sentences, tgt_sentences, max_sent_num = 15):\n",
        "\n",
        "    transformer_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for snt_idx in range(len(src_sentences)):\n",
        "\n",
        "            if snt_idx > max_sent_num:\n",
        "                break\n",
        "\n",
        "            src = src_sentences[snt_idx:snt_idx+1]\n",
        "\n",
        "            padded_src = pad_sequence(src, padding_value=0, batch_first=True).to(device)\n",
        "            src_padding_mask = get_padding_mask(padded_src)\n",
        "            src_subsq_mask = get_square_subsequent_mask(padded_src.size()[1])\n",
        "\n",
        "            snt_translation = transformer_model.translate(\n",
        "                src = padded_src,\n",
        "                tgt_start_code = dataset.get_eng_start_code(),\n",
        "                tgt_eos_code = dataset.get_eng_eos_code(),\n",
        "                src_padding_mask = src_padding_mask,\n",
        "                src_subsq_mask = src_subsq_mask\n",
        "            )\n",
        "\n",
        "            src_sent = ''\n",
        "            for word_idx in src_sentences[snt_idx]:\n",
        "                src_sent = f\"{src_sent} {dataset.fra_token_to_text[word_idx]}\"\n",
        "\n",
        "            tgt_sent = ''\n",
        "            for word_idx in tgt_sentences[snt_idx]:\n",
        "                tgt_sent = f\"{tgt_sent} {dataset.eng_token_to_text[word_idx]}\"\n",
        "\n",
        "            translated_sent = ''\n",
        "            for word_idx in snt_translation:\n",
        "                translated_sent = f\"{translated_sent} {dataset.eng_token_to_text[word_idx]}\"\n",
        "\n",
        "            print(f\"Source sentence is: {src_sent}\")\n",
        "            print(f\"Target sentence is: {tgt_sent}\")\n",
        "            print(f\"Model translation is: {translated_sent}\")\n",
        "\n",
        "    transformer_model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG2e0E8Z_YHT"
      },
      "source": [
        "### Hyperparams, model definition and dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pYECDwr1SuS",
        "outputId": "97f1d4b5-6ab8-4b26-871c-ad8d1e94de4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-GUAsdza6RcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE_XEjKo_YHT",
        "outputId": "52af4179-b0f8-49a1-a644-024d76f14f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "170190\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 2\n",
        "STORE_MODELS = True\n",
        "models_path = 'models'\n",
        "\n",
        "if not os.path.exists(models_path):\n",
        "    os.mkdir(models_path)\n",
        "\n",
        "dataset = FraEngDataset()\n",
        "\n",
        "sentences_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=fra_eng_dataset_collate)\n",
        "\n",
        "in_dict_size = dataset.get_fra_dict_size()\n",
        "out_dict_size = dataset.get_eng_dict_size()\n",
        "\n",
        "transformer_model = Transformer(\n",
        "    num_layers=6,\n",
        "    d_model=512,\n",
        "    num_att_heads=8,\n",
        "    input_dict_size=in_dict_size,\n",
        "    output_dict_size=out_dict_size\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr = 1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izp9Fx2F_YHZ"
      },
      "source": [
        "# Training and sentence translation\n",
        "Now just train the model. You should be able to see the translations at the end of each epoch. Check the quality of your translations. Be patient. Each epoch can take more than 2 hours in a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G35MNimi_YHa",
        "outputId": "4e5b2f95-8db1-4ea8-9dfa-0b498de7ab56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 ============================================================\n",
            "Total loss per word: 2.5541648864746094\n",
            "Some translated sentences:\n",
            "Source sentence is:  <START> Juste en la regardant , on peut voir qu'elle t'aime . <EOS>\n",
            "Target sentence is:  <START> Just looking at her , you can tell that she likes you .\n",
            "Model translation is:  I was in the world , but she can see you . <EOS>\n",
            "Source sentence is:  <START> Tom m ' a dit qu'il n'avait pas peur de Marie . <EOS>\n",
            "Target sentence is:  <START> Tom told me that he was n't afraid of Mary .\n",
            "Model translation is:  Tom told me he was n't afraid of Mary . <EOS>\n",
            "Source sentence is:  <START> Je ne peux me rsoudre  faire une telle chose . <EOS>\n",
            "Target sentence is:  <START> I ca n't bring myself to do such a thing .\n",
            "Model translation is:  I ca n't solve myself to a thing . <EOS>\n",
            "Source sentence is:  <START> Je ne pense pas que j'en ai une . <EOS>\n",
            "Target sentence is:  <START> I do n't think I have one of those .\n",
            "Model translation is:  I do n't think I had a minute . <EOS>\n",
            "Source sentence is:  <START> Nous avons encore beaucoup de temps . <EOS>\n",
            "Target sentence is:  <START> We 've still got a lot of time .\n",
            "Model translation is:  We still have a lot of time . <EOS>\n",
            "Source sentence is:  <START> J'ai bien peur que cette cl ne marche pas . <EOS>\n",
            "Target sentence is:  <START> I 'm afraid this key does n't fit .\n",
            "Model translation is:  I 'm afraid this key would n't be OK . <EOS>\n",
            "Source sentence is:  <START> Je ne suis pas assez bon pour vous . <EOS>\n",
            "Target sentence is:  <START> I 'm not good enough for you .\n",
            "Model translation is:  I 'm not good enough for you . <EOS>\n",
            "Source sentence is:  <START> Je ne ferais pas cela sans vous . <EOS>\n",
            "Target sentence is:  <START> I would n't do that without you .\n",
            "Model translation is:  I would n't do that without you . <EOS>\n",
            "Source sentence is:  <START> Peux-tu t'manciper de tes parents ? <EOS>\n",
            "Target sentence is:  <START> Can you get away from your parents ?\n",
            "Model translation is:  Can you stop your parents ? <EOS>\n",
            "Source sentence is:  <START> Il a encore beaucoup  apprendre . <EOS>\n",
            "Target sentence is:  <START> He still has much to learn .\n",
            "Model translation is:  He has still to learn to learn . <EOS>\n",
            "Source sentence is:  <START> Je ne pourrais pas prendre votre place . <EOS>\n",
            "Target sentence is:  <START> I could n't take your place .\n",
            "Model translation is:  I could n't take your place . <EOS>\n",
            "Source sentence is:  <START> Sautez le plus haut possible . <EOS>\n",
            "Target sentence is:  <START> Jump as high as you can .\n",
            "Model translation is:  Put the most . <EOS>\n",
            "Source sentence is:  <START> L'homme lui a vol son sac . <EOS>\n",
            "Target sentence is:  <START> The man robbed her bag .\n",
            "Model translation is:  The man took him his bag . <EOS>\n",
            "Source sentence is:  <START> tes-vous du coin ? <EOS>\n",
            "Target sentence is:  <START> Are you from around here ?\n",
            "Model translation is:  Are you in the mood ? <EOS>\n",
            "Source sentence is:  <START> Qu'est-ce qui a eu lieu , exactement ? <EOS>\n",
            "Target sentence is:  <START> What exactly happened ?\n",
            "Model translation is:  What happened exactly ? <EOS>\n",
            "Source sentence is:  <START> Vous tes endormie . <EOS>\n",
            "Target sentence is:  <START> You 're sleepy .\n",
            "Model translation is:  You 're beautiful . <EOS>\n",
            "Epoch 1 ============================================================\n",
            "Total loss per word: 1.5537511110305786\n",
            "Some translated sentences:\n",
            "Source sentence is:  <START> Grce aux moyens de communication modernes et aux systmes de transports , le monde a rtrci . <EOS>\n",
            "Target sentence is:  <START> Because of modern communication and transportation systems , the world is getting smaller .\n",
            "Model translation is:  Some of the American and the world are in the world of the world . <EOS>\n",
            "Source sentence is:  <START> Tom a rinc les assiettes et les a mises dans le lave-vaisselle . <EOS>\n",
            "Target sentence is:  <START> Tom rinsed off the plates and put them into the dishwasher .\n",
            "Model translation is:  Tom put the facts and put them into the dishwasher . <EOS>\n",
            "Source sentence is:  <START> De quoi voulez-vous m'entretenir ? <EOS>\n",
            "Target sentence is:  <START> What do you want to talk to me about ?\n",
            "Model translation is:  What do you want to talk about ? <EOS>\n",
            "Source sentence is:  <START> Comment sais-tu de quelle paisseur c'est ? <EOS>\n",
            "Target sentence is:  <START> How do you know how thick it is ?\n",
            "Model translation is:  How do you know what fruit is ? <EOS>\n",
            "Source sentence is:  <START> Il m ' a crit de temps en temps . <EOS>\n",
            "Target sentence is:  <START> He wrote to me from time to time .\n",
            "Model translation is:  He wrote me time in time . <EOS>\n",
            "Source sentence is:  <START> Ne change pas d'avis si souvent . <EOS>\n",
            "Target sentence is:  <START> Do n't change your mind so often .\n",
            "Model translation is:  Let 's not change that often . <EOS>\n",
            "Source sentence is:  <START> Est-ce que Tom tait l quand nous sommes arrivs ? <EOS>\n",
            "Target sentence is:  <START> Was Tom here when we got here ?\n",
            "Model translation is:  Has Tom been there when we were ? <EOS>\n",
            "Source sentence is:  <START> M'aideriez-vous si je dmnage ? <EOS>\n",
            "Target sentence is:  <START> Would you help me if I moved ?\n",
            "Model translation is:  Will I get so much ? <EOS>\n",
            "Source sentence is:  <START> O tiez-vous toute l'aprs-midi ? <EOS>\n",
            "Target sentence is:  <START> Where were you for the whole afternoon ?\n",
            "Model translation is:  Where were you all afternoon ? <EOS>\n",
            "Source sentence is:  <START> Tom sait qui a vol ta voiture . <EOS>\n",
            "Target sentence is:  <START> Tom knows who stole your car .\n",
            "Model translation is:  Tom knows who stole your car . <EOS>\n",
            "Source sentence is:  <START> Vous avez un choix  faire . <EOS>\n",
            "Target sentence is:  <START> You have a choice to make .\n",
            "Model translation is:  You have a choice to do . <EOS>\n",
            "Source sentence is:  <START> Cette cuillre est pour le th . <EOS>\n",
            "Target sentence is:  <START> This spoon is for tea .\n",
            "Model translation is:  This lady is for tea . <EOS>\n",
            "Source sentence is:  <START> Christophe Colomb a dcouvert l'Amrique . <EOS>\n",
            "Target sentence is:  <START> Christopher Columbus discovered America .\n",
            "Model translation is:  The nurse found the boat . <EOS>\n",
            "Source sentence is:  <START> C'est une arnaque . <EOS>\n",
            "Target sentence is:  <START> It 's a scam .\n",
            "Model translation is:  It 's a scam . <EOS>\n",
            "Source sentence is:  <START> Nous aimons parler . <EOS>\n",
            "Target sentence is:  <START> We like to talk .\n",
            "Model translation is:  We 're going to talk . <EOS>\n",
            "Source sentence is:  <START> Je viens de me le rappeler . <EOS>\n",
            "Target sentence is:  <START> I just remembered .\n",
            "Model translation is:  I just got it . <EOS>\n",
            "Training done! Generating some more sentences.\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    src_sentences = None\n",
        "    tgt_sentences = None\n",
        "\n",
        "    train_loss_sum = 0.0\n",
        "    total_word_count = 0.0\n",
        "\n",
        "    for sentences in sentences_loader:\n",
        "\n",
        "        src_sentences = sentences['fra_sentences']\n",
        "        tgt_sentences = sentences['eng_sentences']\n",
        "\n",
        "        tgt_sentences_out = []\n",
        "        for idx in range(len(tgt_sentences)):\n",
        "            tgt_sentences_out.append(tgt_sentences[idx][1:])\n",
        "            tgt_sentences[idx] = tgt_sentences[idx][:-1]\n",
        "\n",
        "        # Create tensors from token lists\n",
        "        padded_src = pad_sequence(src_sentences, padding_value=0, batch_first=True).to(device)\n",
        "        padded_tgt = pad_sequence(tgt_sentences, padding_value=0, batch_first=True).to(device)\n",
        "        padded_tgt_out = pad_sequence(tgt_sentences_out, padding_value=0, batch_first=True).to(device)\n",
        "\n",
        "        src_padding_mask = get_padding_mask(padded_src)\n",
        "        src_subsq_mask = get_square_subsequent_mask(padded_src.size()[1])\n",
        "\n",
        "        tgt_padding_mask = get_padding_mask(padded_tgt)\n",
        "        tgt_subsq_mask = get_square_subsequent_mask(padded_tgt.size()[1])\n",
        "\n",
        "\n",
        "        pred = transformer_model.forward(\n",
        "            src=padded_src,\n",
        "            tgt=padded_tgt,\n",
        "            src_padding_mask=src_padding_mask,\n",
        "            src_subsq_mask=src_subsq_mask,\n",
        "            tgt_padding_mask=tgt_padding_mask,\n",
        "            tgt_subsq_mask=tgt_subsq_mask\n",
        "        )\n",
        "\n",
        "        # Mask to zero one hot vectors corresponding to padded elements\n",
        "        one_hot_mask = get_padding_mask(padded_tgt_out, val1=float(0.0), val2=float(1.0))\n",
        "        y_one_hot = get_one_hot(padded_tgt_out.squeeze(dim=2), out_dict_size, mask=one_hot_mask)\n",
        "\n",
        "        loss = - torch.sum(torch.log(pred) * y_one_hot)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss_sum += loss.detach().to('cpu').data\n",
        "        total_word_count += torch.sum(y_one_hot).to('cpu').data\n",
        "\n",
        "    print(f\"Epoch {epoch} \" + '=' * 60)\n",
        "    print(f\"Total loss per word: {train_loss_sum / total_word_count}\")\n",
        "    print(f\"Some translated sentences:\")\n",
        "    with torch.no_grad():\n",
        "     translate_sentences(src_sentences,tgt_sentences) # Take the last batch and translate some sentences.\n",
        "\n",
        "\n",
        "    if STORE_MODELS == True:\n",
        "        model_path = os.path.join(models_path, f'Epoch_{epoch}_model.pt')\n",
        "        torch.save(transformer_model, model_path)\n",
        "\n",
        "print(\"Training done! Generating some more sentences.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}